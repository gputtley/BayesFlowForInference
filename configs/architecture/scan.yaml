num_coupling_layer:
  - 4
  - 8
units_per_coupling_layer:
  - 64
  - 128
  - 256
num_dense_layers:
  - 4
  - 6
activation: "relu"
dropout: True
mc_dropout: True
epochs: 15
batch_size: 64
early_stopping: True
learning_rate:
  - 0.001
  - 0.01
coupling_design: "interleaved"
permutation: "learnable"
optimizer_name: "Adam"
lr_scheduler_name: "ExponentialDecay"
lr_scheduler_options:
  decay_rate:
    - 0.5
    - 0.8